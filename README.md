


# ğŸ” Advanced RAG Chatbot

A powerful **Retrieval-Augmented Generation (RAG)** system for processing **multiple PDFs** and delivering accurate, **context-aware responses** to user queries. Built in **Python**, it features **GPU-accelerated embeddings**, a **semantic search engine**, and the **Mistral model via Ollama**, wrapped in an intuitive **Streamlit interface**.

---

## ğŸš€ Features

* ğŸ“„ **Multi-PDF Support** â€“ Upload and process several PDFs at once.
* âš¡ **GPU-Accelerated Embeddings** â€“ Leverage NVIDIA GPUs for fast processing.
* ğŸ” **Semantic Search** â€“ Uses FAISS for efficient context retrieval with source tracking.
* ğŸŒ **Streamlit Interface** â€“ Clean and interactive web UI for document upload, chat, and performance.
* ğŸ§© **Highly Configurable** â€“ Adjust chunk size, overlap, batch size, and worker threads.
* ğŸ“Š **Real-time Metrics** â€“ Monitor chunking speed, memory usage, and system stats.
* ğŸ’¬ **Chat History Export** â€“ Save and download all chat sessions in JSON format.

---

## ğŸ“ Repository Info

| Field             | Details                                                                   |
| ----------------- | ------------------------------------------------------------------------- |
| **Repo Name**     | [Advanced-RAG-Chatbot](https://github.com/milind899/Advanced-RAG-Chatbot) |
| **License**       | MIT                                                                       |
| **Language**      | Python                                                                    |
| **Model**         | [Mistral](https://ollama.ai/library/mistral) via Ollama                   |
| **Embeddings**    | `intfloat/e5-small` (HuggingFace)                                         |
| **Interface**     | Streamlit                                                                 |
| **Vector Store**  | FAISS                                                                     |
| **PDF Processor** | PyMuPDF                                                                   |
| **Status**        | ğŸš§ Actively maintained                                                    |

---

## ğŸ–¥ï¸ Requirements

### âœ… Hardware

| Type           | Minimum | Recommended                   |
| -------------- | ------- | ----------------------------- |
| CPU            | 4-core  | 8-core                        |
| RAM            | 8 GB    | 16 GB                         |
| Storage        | 10 GB   | 20 GB                         |
| GPU (Optional) | âŒ       | âœ… NVIDIA (GTX 1060 or higher) |

### âœ… Software

* Python 3.8+
* Git
* CUDA Toolkit 11.2+ (for GPU acceleration)
* Ollama (â‰¥ 0.1.0)

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Install Python

```bash
# Download Python from https://python.org
python --version  # âœ… Ensure it's â‰¥ 3.8
```

### 2ï¸âƒ£ Install Git

```bash
# Download from https://git-scm.com
git --version
```

### 3ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/milind899/Advanced-RAG-Chatbot.git
cd Advanced-RAG-Chatbot
```

### 4ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate
```

### 5ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

#### ğŸ“¦ Key Libraries

* `streamlit` â€“ Frontend UI
* `langchain` â€“ RAG orchestration
* `torch` â€“ Model + CUDA acceleration
* `transformers` â€“ Embedding generation
* `faiss-cpu` â€“ Vector similarity search
* `PyMuPDF` â€“ PDF parser

---

## ğŸ¤– Ollama Setup

### Install Ollama

```bash
curl -fsSL https://ollama.ai/install.sh | sh
ollama pull mistral
ollama serve
```

### Verify Setup

```bash
curl http://localhost:11434/api/tags
```

### Enable GPU (Optional)

```bash
# Verify CUDA
nvidia-smi

# Enable GPU
unset OLLAMA_NO_CUDA

# Test with PyTorch
python -c "import torch; print(torch.cuda.is_available())"
```

---

## ğŸš€ Run the Application

```bash
streamlit run app.py
```

> ğŸ”— Open your browser at: [http://localhost:8501](http://localhost:8501)

---

## ğŸ’¡ How to Use

### ğŸ“‚ Uploading PDFs

1. Open the app in your browser.
2. Drag-and-drop or browse to upload PDFs.
3. Adjust settings from the sidebar:

   * `Chunk Size` (default: 500)
   * `Overlap` (default: 50)
   * `Workers` (CPU count default)
   * `Batch Size` (default: 100)
4. Click **Process Documents**.

### â“ Asking Questions

* Go to the **Chat with Your Documents** section.
* Type a question and hit **Send**.
* View responses with **source links**.
* Review **chat history** at the bottom.
* Export your chats via the **Export JSON** button.

### ğŸ“ˆ Monitoring & Stats

* View chunks/sec, total time in **Performance Metrics**.
* Sidebar shows **GPU & system usage**.
* Troubleshoot with real-time logs.

---

## ğŸ—‚ï¸ Project Structure

```
Advanced-RAG-Chatbot/
â”œâ”€â”€ app.py              # Streamlit UI
â”œâ”€â”€ rag_backend.py      # Core RAG logic
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ LICENSE             # MIT License
â”œâ”€â”€ faiss_index/        # Generated vector store
â”œâ”€â”€ embedding_cache/    # Cached embeddings
```

---

## âš™ï¸ Configuration Options (in `rag_backend.py`)

| Parameter          | Description                   | Default           |
| ------------------ | ----------------------------- | ----------------- |
| `chunk_size`       | Text chunk size               | 500               |
| `chunk_overlap`    | Overlap between chunks        | 50                |
| `max_workers`      | Concurrent threads            | CPU count         |
| `batch_size`       | Documents per embedding batch | 100               |
| `embedding_model`  | HuggingFace model             | intfloat/e5-small |
| `cache_embeddings` | Use embedding cache           | True              |

---

## ğŸ§¯ Troubleshooting

| Problem                 | Fix                                                            |
| ----------------------- | -------------------------------------------------------------- |
| âŒ Ollama not responding | Ensure `ollama serve` is running and model is pulled.          |
| âŒ CUDA not available    | Install CUDA Toolkit, check `nvidia-smi`, enable with `unset`. |
| ğŸ’¥ Memory crash         | Reduce `batch_size` or `chunk_size`, clear cache folders.      |
| ğŸ¢ Slow performance     | Enable GPU, increase `max_workers`, tune chunking strategy.    |

---

## ğŸ“¬ Support

* ğŸ”§ Use the **Help & Tips** section in the app.
* ğŸ› File issues or suggestions on [GitHub Issues](https://github.com/milind899/Advanced-RAG-Chatbot/issues)
* ğŸ“„ Refer to `rag_backend.py` for logic and model details.

---

## ğŸ“œ License

This project is licensed under the **MIT License**. See [LICENSE](LICENSE) for more information.


